% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rgcca_cv.r
\name{rgcca_cv}
\alias{rgcca_cv}
\title{Tune RGCCA parameters in 'supervised' mode with cross-validation}
\usage{
rgcca_cv(
  blocks,
  type = "rgcca",
  response = NULL,
  par_type = "tau",
  par_value = NULL,
  par_length = 10,
  validation = "kfold",
  type_cv = "regression",
  fit = "lm",
  k = 5,
  n_run = 1,
  one_value_per_cv = FALSE,
  n_cores = parallel::detectCores() - 1,
  quiet = TRUE,
  superblock = FALSE,
  scale = TRUE,
  scale_block = TRUE,
  tol = 1e-08,
  scheme = "factorial",
  method = "nipals",
  rgcca_res = NULL,
  parallelization = NULL,
  tau = rep(1, length(blocks)),
  ncomp = rep(1, length(blocks)),
  sparsity = rep(1, length(blocks)),
  init = "svd",
  bias = TRUE,
  new_scaled = FALSE,
  ...
)
}
\arguments{
\item{blocks}{A list that contains the J blocks of variables X1, X2, ..., XJ.
Block Xj is a matrix of dimension n x p_j where p_j is the number of
variables in X_j.}

\item{type}{A character string indicating the multi-block component
method to consider: rgcca, sgcca, pca, spca, pls, spls, cca,
ifa, ra, gcca, maxvar, maxvar-b, maxvar-a, mcoa,cpca-1, cpca-2,
cpca-4, hpca, maxbet-b, maxbet, maxdiff-b, maxdiff, maxvar-a,
sabscor, ssqcor, ssqcor, ssqcov-1, ssqcov-2, ssqcov, sumcor,
sumcov-1, sumcov-2, sumcov, sabscov, sabscov-1, sabscov-2.}

\item{response}{An integer giving the position of the response block. When
the response argument is filled the supervised mode is automatically
activated.}

\item{par_type}{A character giving the parameter to tune among "sparsity" or "tau".}

\item{par_value}{A matrix (n*p, with p the number of blocks and n the number
of combinations to be tested), a vector (of p length) or a numeric value
giving sets of penalties (tau for RGCCA, sparsity for SGCCA) to be tested,
one row by combination. By default, it takes 10 sets between min values (0
 for RGCCA and $1/sqrt(ncol)$ for SGCCA) and 1.}

\item{par_length}{An integer indicating the number of sets of parameters to be tested (if par_value = NULL). The parameters are uniformly distributed.}

\item{validation}{A character for the type of validation among "loo", "kfold", "test".}

\item{type_cv}{A character corresponding to the model of prediction : 'regression' or 'classification' (see details)}

\item{fit}{A character giving the function used to compare the trained and the tested models}

\item{k}{An integer giving the number of folds (if validation = 'kfold').}

\item{n_run}{An integer giving the number of cross-validations to be run (if validation = 'kfold').}

\item{one_value_per_cv}{A logical value indicating if the k values are averaged for each k-fold steps.}

\item{n_cores}{Number of cores for parallelization}

\item{quiet}{A logical value indicating if the warning messages are reported.}

\item{superblock}{Boolean indicating the presence of the superblock.
Default = TRUE}

\item{scale}{A logical value indicating if each block is standardized}

\item{scale_block}{A logical value indicating if each block is divided by
the square root of its number of variables.}

\item{tol}{The stopping value for the convergence of the algorithm.}

\item{scheme}{A character string or a function giving the scheme function for
covariance maximization among "horst" (the identity function), "factorial"
 (the squared values), "centroid" (the absolute values). The scheme function
 can be any continously differentiable convex functin and it is possible to
 design explicitely the sheme function (e.g. function(x) x^4) as argument of
 rgcca function.  See (Tenenhaus et al, 2017) for details.}

\item{method}{Either a character corresponding to the used method
("complete","knn","em","sem") or a function taking a list of J blocks (A) as
only parameter and returning the imputed list.
\itemize{
\item{\code{"mean"}}{ corresponds to an imputation by the colmeans}
\item{\code{"complete"}}{ corresponds to run RGCCA only on the complete
subjects (subjects with missing data are removed)}
\item{\code{"nipals"}}{ corresponds to run RGCCA on all available data
(NIPALS algorithm)}
\item{\code{"em"}}{ corresponds to impute the data with EM-type algorithms}
\item{\code{"sem"}}{ corresponds to impute the data with EM-type algorithms
with superblock approach}
\item{\code{"knn1"}}{ corresponds to impute the data with the 1-Nearest
Neighbor. 1 can be replace by another number (such as knn3) to impute with
the 3-Nearest Neighbors.}}}

\item{rgcca_res}{A fitted RGCCA object (see  \code{\link[RGCCA]{rgcca}})}

\item{parallelization}{logical value. If TRUE (default value), the
cross-validation procedure is parallelized}

\item{tau}{Either a 1*J vector or a \eqn{\mathrm{max}(ncomp) \times J} matrix
containing the values of the regularization parameters (default: tau = 1,
for each block and each dimension). Tau varies from 0 (maximizing the
correlation) to 1 (maximizing the covariance). If tau = "optimal" the
regularization paramaters are estimated for each block and each dimension
using the Schafer and Strimmer (2005) analytical formula . If tau is a
\eqn{1\times J} vector, tau[j] is identical across the dimensions of block
\eqn{\mathbf{X}_j}. If tau is a matrix, tau[k, j] is associated with
\eqn{\mathbf{X}_{jk}} (\eqn{k}th residual matrix for block \eqn{j}). It can
be estimated by using \link{rgcca_permutation}.}

\item{ncomp}{A vector of length J indicating the number of block components
for each block.}

\item{sparsity}{Either a \eqn{1*J} vector or a \eqn{max(ncomp) * J} matrix 
encoding the L1 constraints applied to the outer weight vectors. The amount 
of sparsity varies between \eqn{1/sqrt(p_j)} and 1 (larger values of sparsity 
correspond to less penalization). If sparsity is a vector, L1-penalties are 
the same for all the weights corresponding to the same block but different 
components: 
\deqn{for all h, |a_{j,h}|_{L_1} \le c_1[j] \sqrt{p_j},}
with \eqn{p_j} the number of variables of \eqn{X_j}.
If sparsity is a matrix, each row \eqn{h} defines the constraints applied to 
the weights corresponding to components \eqn{h}:
\deqn{for all h, |a_{j,h}|_{L_1} \le c_1[h,j] \sqrt{p_j}.} It can be 
estimated by using \link{rgcca_permutation}.}

\item{init}{A character giving the mode of initialization to use in the
algorithm. The alternatives are either by Singular Value Decompostion ("svd")
or random ("random") (default: "svd").}

\item{bias}{A logical value for biaised (\eqn{1/n}) or unbiaised
(\eqn{1/(n-1)}) estimator of the var/cov (default: bias = TRUE).}

\item{new_scaled}{A boolean scaling the blocks to predict}

\item{...}{Further graphical parameters (see plot2D functions)}
}
\value{
\item{cv}{A matrix giving the root-mean-square error (RMSE) between the predicted R/SGCCA and the observed R/SGCCA for each combination and each prediction (n_prediction = n_samples for validation = 'loo'; n_prediction = 'k' * 'n_run' for validation = 'kfold').}

\item{call}{A list of the input parameters}

\item{bestpenalties}{Penalties giving the best RMSE for each blocks (for regression) or the best proportion of wrong predictions (for classification)}

\item{penalties}{A matrix giving, for each blocks, the penalty combinations (tau or sparsity)}
}
\description{
Tune the sparsity coefficient (if the model is sparse) or tau
(otherwise) in a supervised approach by estimating by crossvalidation the predictive quality of the models.
In this purpose, the samples are divided into k folds where the model will be tested on each fold and trained
 on the others. For small datasets (<30 samples), it is recommended to use
 as many folds as there are individuals (leave-one-out; loo).
}
\details{
If type_cv=="regression",at each round of cross-validation, for each variable, a predictive model is constructed as a linear model of the first RGCCA component of each block (calculated on the training set).
 Then the Root Mean Square of Errors of this model on the testing dataset are calculated, then averaged on the variables of the predictive block.
 The best combination of parameters is the one where the average of RMSE on the testing datasets is the lowest.
If type_cv=="classification", at each round of cross-validation a "lda" is run and the proportion of wrong predictions on the testing dataset is returned.
}
\examples{
data("Russett")
blocks <- list(
  agriculture = Russett[, seq(3)],
  industry = Russett[, 4:5],
  politic = Russett[, 6:11]
)
res <- rgcca_cv(blocks,
  response = 3, type = "rgcca",
  par_type = "sparsity",
  par_value = c(0.6, 0.75, 0.5),
  n_run = 2, n_cores = 1
)
plot(res)
rgcca_cv(blocks,
  response = 3, par_type = "tau",
  par_value = c(0.6, 0.75, 0.5),
  n_run = 2, n_cores = 1
)$bestpenalties

rgcca_cv(blocks,
  response = 3, par_type = "sparsity",
  par_value = 0.8, n_run = 2, n_cores = 1
)

rgcca_cv(blocks,
  response = 3, par_type = "tau",
  par_value = 0.8, n_run = 2, n_cores = 1
)

}
